{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import gensim\n",
    "import neural_net\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_df.sentence.values\n",
    "train_labels = train_df.label.values\n",
    "\n",
    "test_sentences = test_df.sentence.values\n",
    "test_labels = test_df.label.values\n",
    "\n",
    "num_words = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('../model/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bring_nn_input(sentences):\n",
    "    vec_list = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = sentence.strip().split()\n",
    "\n",
    "        count = 0\n",
    "        sum_vec = np.zeros(300)\n",
    "\n",
    "        for word in words:\n",
    "            if word in word2vec_model:\n",
    "                sum_vec += word2vec_model[word]\n",
    "                count += 1\n",
    "        \n",
    "        if count != 0:\n",
    "            vec = (sum_vec / count)\n",
    "    \n",
    "        vec_list.append(vec)\n",
    "\n",
    "    return np.array(vec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = bring_nn_input(train_sentences)\n",
    "test_vec = bring_nn_input(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input, hidden, output):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = nn.Linear(input, hidden)\n",
    "        self.l2 = nn.Linear(hidden , hidden)\n",
    "        self.l3 = nn.Linear(hidden, output)\n",
    "\n",
    "        # self.linear1 = torch.nn.Linear(input, hidden)\n",
    "        # self.activation = torch.nn.ReLU()\n",
    "        # self.linear2 = torch.nn.Linear(hidden, output)\n",
    "        # self.softmax = torch.nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.l1(x))\n",
    "        out = F.relu(self.l2(out))\n",
    "        out = self.l3(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        # x = self.linear1(x)\n",
    "        # x = self.activation(x)\n",
    "        # x = self.linear2(x)\n",
    "        # x = self.softmax(x)\n",
    "\n",
    "        # return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 300\n",
    "hidden=100\n",
    "output = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input, hidden, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(train_vec, train_labels, shuffle=True, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    x_train = Variable(torch.from_numpy(features_train)).float()\n",
    "    y_train = Variable(torch.from_numpy(labels_train)).long()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_train)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        print (\"epoch #\",epoch)\n",
    "        print (\"loss: \", loss.item())\n",
    "        pred = torch.max(y_pred, 1)[1].eq(y_train).sum()\n",
    "        print (\"acc:(%) \", 100*pred/len(x_train))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epochs):\n",
    "    model.eval()\n",
    "    x_test = Variable(torch.from_numpy(features_test)).float()\n",
    "    y_test = Variable(torch.from_numpy(labels_test)).long()\n",
    "    for epoch in range(epochs):\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x_test)\n",
    "            loss = criterion(y_pred, y_test)\n",
    "            print (\"epoch #\",epoch)\n",
    "            print (\"loss: \", loss.item())\n",
    "            pred = torch.max(y_pred, 1)[1].eq(y_test).sum()\n",
    "            print (\"acc (%): \", 100*pred/len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x0/8r0rxn1n0zz_ffhdl9_ctgq40000gn/T/ipykernel_22555/3057272900.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch # 0\n",
      "loss:  0.4574134647846222\n",
      "acc:(%)  tensor(85.8683)\n",
      "epoch # 1\n",
      "loss:  0.45703208446502686\n",
      "acc:(%)  tensor(85.9688)\n",
      "epoch # 2\n",
      "loss:  0.4567093551158905\n",
      "acc:(%)  tensor(85.9688)\n",
      "epoch # 3\n",
      "loss:  0.4563997983932495\n",
      "acc:(%)  tensor(86.0510)\n",
      "epoch # 4\n",
      "loss:  0.45605918765068054\n",
      "acc:(%)  tensor(86.0144)\n",
      "epoch # 5\n",
      "loss:  0.4557071328163147\n",
      "acc:(%)  tensor(86.1332)\n",
      "epoch # 6\n",
      "loss:  0.45528513193130493\n",
      "acc:(%)  tensor(86.0875)\n",
      "epoch # 7\n",
      "loss:  0.4548576772212982\n",
      "acc:(%)  tensor(86.1971)\n",
      "epoch # 8\n",
      "loss:  0.4544161856174469\n",
      "acc:(%)  tensor(86.2154)\n",
      "epoch # 9\n",
      "loss:  0.45400989055633545\n",
      "acc:(%)  tensor(86.3159)\n",
      "epoch # 10\n",
      "loss:  0.45364469289779663\n",
      "acc:(%)  tensor(86.3067)\n",
      "epoch # 11\n",
      "loss:  0.453305721282959\n",
      "acc:(%)  tensor(86.3159)\n",
      "epoch # 12\n",
      "loss:  0.4530113935470581\n",
      "acc:(%)  tensor(86.4712)\n",
      "epoch # 13\n",
      "loss:  0.4527553617954254\n",
      "acc:(%)  tensor(86.3524)\n",
      "epoch # 14\n",
      "loss:  0.4525650441646576\n",
      "acc:(%)  tensor(86.4438)\n",
      "epoch # 15\n",
      "loss:  0.45239296555519104\n",
      "acc:(%)  tensor(86.4255)\n",
      "epoch # 16\n",
      "loss:  0.4521845579147339\n",
      "acc:(%)  tensor(86.4712)\n",
      "epoch # 17\n",
      "loss:  0.4517335295677185\n",
      "acc:(%)  tensor(86.5351)\n",
      "epoch # 18\n",
      "loss:  0.45113080739974976\n",
      "acc:(%)  tensor(86.5991)\n",
      "epoch # 19\n",
      "loss:  0.4505942165851593\n",
      "acc:(%)  tensor(86.6996)\n",
      "epoch # 20\n",
      "loss:  0.45032256841659546\n",
      "acc:(%)  tensor(86.7361)\n",
      "epoch # 21\n",
      "loss:  0.4502178430557251\n",
      "acc:(%)  tensor(86.7909)\n",
      "epoch # 22\n",
      "loss:  0.45005542039871216\n",
      "acc:(%)  tensor(86.7635)\n",
      "epoch # 23\n",
      "loss:  0.4497303068637848\n",
      "acc:(%)  tensor(86.8183)\n",
      "epoch # 24\n",
      "loss:  0.44920769333839417\n",
      "acc:(%)  tensor(86.8457)\n",
      "epoch # 25\n",
      "loss:  0.4487163722515106\n",
      "acc:(%)  tensor(86.9827)\n",
      "epoch # 26\n",
      "loss:  0.44837790727615356\n",
      "acc:(%)  tensor(87.0010)\n",
      "epoch # 27\n",
      "loss:  0.44818517565727234\n",
      "acc:(%)  tensor(87.0010)\n",
      "epoch # 28\n",
      "loss:  0.4480445086956024\n",
      "acc:(%)  tensor(87.0010)\n",
      "epoch # 29\n",
      "loss:  0.4478142559528351\n",
      "acc:(%)  tensor(87.0467)\n",
      "epoch # 30\n",
      "loss:  0.4474754333496094\n",
      "acc:(%)  tensor(87.0467)\n",
      "epoch # 31\n",
      "loss:  0.4470388889312744\n",
      "acc:(%)  tensor(87.1380)\n",
      "epoch # 32\n",
      "loss:  0.446623295545578\n",
      "acc:(%)  tensor(87.1928)\n",
      "epoch # 33\n",
      "loss:  0.44629159569740295\n",
      "acc:(%)  tensor(87.2659)\n",
      "epoch # 34\n",
      "loss:  0.4460466802120209\n",
      "acc:(%)  tensor(87.2933)\n",
      "epoch # 35\n",
      "loss:  0.4458545744419098\n",
      "acc:(%)  tensor(87.3025)\n",
      "epoch # 36\n",
      "loss:  0.44568079710006714\n",
      "acc:(%)  tensor(87.3025)\n",
      "epoch # 37\n",
      "loss:  0.4454782009124756\n",
      "acc:(%)  tensor(87.2751)\n",
      "epoch # 38\n",
      "loss:  0.44522377848625183\n",
      "acc:(%)  tensor(87.3207)\n",
      "epoch # 39\n",
      "loss:  0.4449279308319092\n",
      "acc:(%)  tensor(87.3116)\n",
      "epoch # 40\n",
      "loss:  0.4445679187774658\n",
      "acc:(%)  tensor(87.4029)\n",
      "epoch # 41\n",
      "loss:  0.4442135989665985\n",
      "acc:(%)  tensor(87.4852)\n",
      "epoch # 42\n",
      "loss:  0.4438956379890442\n",
      "acc:(%)  tensor(87.5034)\n",
      "epoch # 43\n",
      "loss:  0.4436347484588623\n",
      "acc:(%)  tensor(87.5308)\n",
      "epoch # 44\n",
      "loss:  0.44340452551841736\n",
      "acc:(%)  tensor(87.5856)\n",
      "epoch # 45\n",
      "loss:  0.4432078003883362\n",
      "acc:(%)  tensor(87.5308)\n",
      "epoch # 46\n",
      "loss:  0.4430336356163025\n",
      "acc:(%)  tensor(87.6130)\n",
      "epoch # 47\n",
      "loss:  0.4428703784942627\n",
      "acc:(%)  tensor(87.6679)\n",
      "epoch # 48\n",
      "loss:  0.44272100925445557\n",
      "acc:(%)  tensor(87.6587)\n",
      "epoch # 49\n",
      "loss:  0.4425351321697235\n",
      "acc:(%)  tensor(87.5948)\n",
      "epoch # 50\n",
      "loss:  0.44230008125305176\n",
      "acc:(%)  tensor(87.6496)\n",
      "epoch # 51\n",
      "loss:  0.44200408458709717\n",
      "acc:(%)  tensor(87.6496)\n",
      "epoch # 52\n",
      "loss:  0.44164493680000305\n",
      "acc:(%)  tensor(87.7318)\n",
      "epoch # 53\n",
      "loss:  0.4412856996059418\n",
      "acc:(%)  tensor(87.8323)\n",
      "epoch # 54\n",
      "loss:  0.44097480177879333\n",
      "acc:(%)  tensor(87.7866)\n",
      "epoch # 55\n",
      "loss:  0.4407193958759308\n",
      "acc:(%)  tensor(87.8049)\n",
      "epoch # 56\n",
      "loss:  0.4405227303504944\n",
      "acc:(%)  tensor(87.7957)\n",
      "epoch # 57\n",
      "loss:  0.44035857915878296\n",
      "acc:(%)  tensor(87.8871)\n",
      "epoch # 58\n",
      "loss:  0.44021695852279663\n",
      "acc:(%)  tensor(87.9236)\n",
      "epoch # 59\n",
      "loss:  0.4401116669178009\n",
      "acc:(%)  tensor(87.9419)\n",
      "epoch # 60\n",
      "loss:  0.4400411546230316\n",
      "acc:(%)  tensor(87.9054)\n",
      "epoch # 61\n",
      "loss:  0.44007232785224915\n",
      "acc:(%)  tensor(87.8780)\n",
      "epoch # 62\n",
      "loss:  0.43998247385025024\n",
      "acc:(%)  tensor(87.8871)\n",
      "epoch # 63\n",
      "loss:  0.4397347867488861\n",
      "acc:(%)  tensor(87.9145)\n",
      "epoch # 64\n",
      "loss:  0.4391816556453705\n",
      "acc:(%)  tensor(87.9693)\n",
      "epoch # 65\n",
      "loss:  0.43863195180892944\n",
      "acc:(%)  tensor(88.0607)\n",
      "epoch # 66\n",
      "loss:  0.4383085370063782\n",
      "acc:(%)  tensor(88.0607)\n",
      "epoch # 67\n",
      "loss:  0.4382439851760864\n",
      "acc:(%)  tensor(88.0424)\n",
      "epoch # 68\n",
      "loss:  0.438288152217865\n",
      "acc:(%)  tensor(88.0607)\n",
      "epoch # 69\n",
      "loss:  0.438256174325943\n",
      "acc:(%)  tensor(88.0241)\n",
      "epoch # 70\n",
      "loss:  0.4380378723144531\n",
      "acc:(%)  tensor(88.0789)\n",
      "epoch # 71\n",
      "loss:  0.4376010596752167\n",
      "acc:(%)  tensor(88.0789)\n",
      "epoch # 72\n",
      "loss:  0.4371795654296875\n",
      "acc:(%)  tensor(88.1794)\n",
      "epoch # 73\n",
      "loss:  0.43693268299102783\n",
      "acc:(%)  tensor(88.1977)\n",
      "epoch # 74\n",
      "loss:  0.4368593394756317\n",
      "acc:(%)  tensor(88.2799)\n",
      "epoch # 75\n",
      "loss:  0.43686676025390625\n",
      "acc:(%)  tensor(88.2890)\n",
      "epoch # 76\n",
      "loss:  0.43682050704956055\n",
      "acc:(%)  tensor(88.1703)\n",
      "epoch # 77\n",
      "loss:  0.4366367757320404\n",
      "acc:(%)  tensor(88.2342)\n",
      "epoch # 78\n",
      "loss:  0.43630078434944153\n",
      "acc:(%)  tensor(88.2799)\n",
      "epoch # 79\n",
      "loss:  0.4359533190727234\n",
      "acc:(%)  tensor(88.3712)\n",
      "epoch # 80\n",
      "loss:  0.4356807768344879\n",
      "acc:(%)  tensor(88.3621)\n",
      "epoch # 81\n",
      "loss:  0.43549972772598267\n",
      "acc:(%)  tensor(88.3987)\n",
      "epoch # 82\n",
      "loss:  0.4353989362716675\n",
      "acc:(%)  tensor(88.4169)\n",
      "epoch # 83\n",
      "loss:  0.43533942103385925\n",
      "acc:(%)  tensor(88.4078)\n",
      "epoch # 84\n",
      "loss:  0.43529289960861206\n",
      "acc:(%)  tensor(88.4169)\n",
      "epoch # 85\n",
      "loss:  0.4352196455001831\n",
      "acc:(%)  tensor(88.3804)\n",
      "epoch # 86\n",
      "loss:  0.43510231375694275\n",
      "acc:(%)  tensor(88.3804)\n",
      "epoch # 87\n",
      "loss:  0.43487927317619324\n",
      "acc:(%)  tensor(88.4169)\n",
      "epoch # 88\n",
      "loss:  0.434621125459671\n",
      "acc:(%)  tensor(88.4900)\n",
      "epoch # 89\n",
      "loss:  0.4343273937702179\n",
      "acc:(%)  tensor(88.5722)\n",
      "epoch # 90\n",
      "loss:  0.43405771255493164\n",
      "acc:(%)  tensor(88.5631)\n",
      "epoch # 91\n",
      "loss:  0.43383827805519104\n",
      "acc:(%)  tensor(88.6362)\n",
      "epoch # 92\n",
      "loss:  0.4336634576320648\n",
      "acc:(%)  tensor(88.6453)\n",
      "epoch # 93\n",
      "loss:  0.43352875113487244\n",
      "acc:(%)  tensor(88.6087)\n",
      "epoch # 94\n",
      "loss:  0.43341881036758423\n",
      "acc:(%)  tensor(88.6544)\n",
      "epoch # 95\n",
      "loss:  0.433351069688797\n",
      "acc:(%)  tensor(88.6727)\n",
      "epoch # 96\n",
      "loss:  0.43332982063293457\n",
      "acc:(%)  tensor(88.7732)\n",
      "epoch # 97\n",
      "loss:  0.43336623907089233\n",
      "acc:(%)  tensor(88.5996)\n",
      "epoch # 98\n",
      "loss:  0.4334448277950287\n",
      "acc:(%)  tensor(88.6270)\n",
      "epoch # 99\n",
      "loss:  0.4335658848285675\n",
      "acc:(%)  tensor(88.5448)\n"
     ]
    }
   ],
   "source": [
    "train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x0/8r0rxn1n0zz_ffhdl9_ctgq40000gn/T/ipykernel_22555/3057272900.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch # 0\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 1\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 2\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 3\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 4\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 5\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 6\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 7\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 8\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 9\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 10\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 11\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 12\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 13\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 14\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 15\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 16\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 17\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 18\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 19\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 20\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 21\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 22\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 23\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 24\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 25\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 26\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 27\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 28\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 29\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 30\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 31\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 32\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 33\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 34\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 35\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 36\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 37\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 38\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 39\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 40\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 41\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 42\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 43\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 44\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 45\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 46\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 47\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 48\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 49\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 50\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 51\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 52\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 53\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 54\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 55\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 56\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 57\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 58\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 59\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 60\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 61\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 62\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 63\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 64\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 65\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 66\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 67\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 68\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 69\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 70\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 71\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 72\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 73\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 74\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 75\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 76\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 77\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 78\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 79\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 80\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 81\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 82\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 83\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 84\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 85\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 86\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 87\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 88\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 89\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 90\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 91\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 92\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 93\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 94\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 95\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 96\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 97\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 98\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n",
      "epoch # 99\n",
      "loss:  0.4942275881767273\n",
      "acc (%):  tensor(80.8715)\n"
     ]
    }
   ],
   "source": [
    "test(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
