{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import gensim\n",
    "import neural_net\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_df.sentence.values\n",
    "train_labels = train_df.label.values\n",
    "\n",
    "test_sentences = test_df.sentence.values\n",
    "test_labels = test_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('../model/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bring_nn_input(sentences):\n",
    "    vec_list = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = sentence.strip().split()\n",
    "\n",
    "        count = 0\n",
    "        sum_vec = np.zeros(300)\n",
    "\n",
    "        for word in words:\n",
    "            if word in word2vec_model:\n",
    "                sum_vec += word2vec_model[word]\n",
    "                count += 1\n",
    "        \n",
    "        if count != 0:\n",
    "            vec = (sum_vec / count)\n",
    "    \n",
    "        vec_list.append(vec)\n",
    "\n",
    "    return np.array(vec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = bring_nn_input(train_sentences)\n",
    "test_vec = bring_nn_input(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 300\n",
    "hidden=100\n",
    "output = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neural_net.Model(input, hidden, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(train_vec, train_labels, shuffle=True, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    x_train = Variable(torch.from_numpy(features_train)).float()\n",
    "    y_train = Variable(torch.from_numpy(labels_train)).long()\n",
    "\n",
    "    x_test = Variable(torch.from_numpy(test_vec)).float()\n",
    "    y_test = Variable(torch.from_numpy(test_labels)).long()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred_train = model(x_train)\n",
    "        loss_train = criterion(y_pred_train, y_train)\n",
    "        \n",
    "        # print (\"epoch #\",epoch)\n",
    "        print (\"train loss: \", loss_train.item())\n",
    "        pred_train = torch.max(y_pred_train, 1)[1].eq(y_train).sum()\n",
    "        print (\"train acc:(%) \", 100*pred_train/len(x_train))\n",
    "\n",
    "        tb_x = epoch\n",
    "        # writer.add_scalar('Loss/train', loss_train.item(), tb_x)\n",
    "\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred_test = model(x_test)\n",
    "            loss_test = criterion(y_pred_test, y_test)\n",
    "            \n",
    "            # print (\"epoch #\",epoch)\n",
    "            print (\"test loss: \", loss_test.item())\n",
    "            pred_test = torch.max(y_pred_test, 1)[1].eq(y_test).sum()\n",
    "            print (\"test acc (%): \", 100*pred_test/len(x_test))\n",
    "        \n",
    "        writer.add_scalars('Training vs. Testing Loss',\n",
    "                    { 'Training' : loss_train.item(), 'Validation' : loss_test.item() },\n",
    "                    tb_x + 1)\n",
    "        \n",
    "        writer.add_scalars('Training vs. Testing Accuracy',\n",
    "                    { 'Training' : 100*pred_train/len(x_train), 'Validation' : 100*pred_test/len(x_test) },\n",
    "                    tb_x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epochs):\n",
    "    model.eval()\n",
    "    x_test = Variable(torch.from_numpy(features_test)).float()\n",
    "    y_test = Variable(torch.from_numpy(labels_test)).long()\n",
    "    for epoch in range(epochs):\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x_test)\n",
    "            loss = criterion(y_pred, y_test)\n",
    "            print (\"epoch #\",epoch)\n",
    "            print (\"loss: \", loss.item())\n",
    "            pred = torch.max(y_pred, 1)[1].eq(y_test).sum()\n",
    "            print (\"acc (%): \", 100*pred/len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  0.710987389087677\n",
      "train acc:(%)  tensor(37.7638)\n",
      "test loss:  0.6935567259788513\n",
      "test acc (%):  tensor(50.)\n"
     ]
    }
   ],
   "source": [
    "train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch # 0\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 1\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 2\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 3\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 4\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 5\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 6\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 7\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 8\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 9\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 10\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 11\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 12\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 13\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 14\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 15\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 16\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 17\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 18\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 19\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 20\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 21\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 22\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 23\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 24\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 25\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 26\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 27\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 28\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 29\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 30\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 31\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 32\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 33\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 34\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 35\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 36\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 37\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 38\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 39\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 40\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 41\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 42\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 43\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 44\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 45\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 46\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 47\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 48\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 49\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 50\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 51\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 52\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 53\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 54\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 55\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 56\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 57\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 58\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 59\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 60\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 61\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 62\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 63\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 64\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 65\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 66\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 67\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 68\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 69\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 70\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 71\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 72\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 73\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 74\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 75\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 76\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 77\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 78\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 79\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 80\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 81\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 82\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 83\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 84\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 85\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 86\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 87\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 88\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 89\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 90\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 91\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 92\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 93\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 94\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 95\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 96\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 97\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 98\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n",
      "epoch # 99\n",
      "loss:  0.6700409054756165\n",
      "acc (%):  tensor(61.7978)\n"
     ]
    }
   ],
   "source": [
    "test(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_test(epochs):\n",
    "    model.eval()\n",
    "    x_test = Variable(torch.from_numpy(test_vec)).float()\n",
    "    y_test = Variable(torch.from_numpy(test_labels)).long()\n",
    "    for epoch in range(epochs):\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x_test)\n",
    "            loss = criterion(y_pred, y_test)\n",
    "            print (\"epoch #\",epoch)\n",
    "            print (\"loss: \", loss.item())\n",
    "            pred = torch.max(y_pred, 1)[1].eq(y_test).sum()\n",
    "            print (\"acc (%): \", 100*pred/len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch # 0\n",
      "loss:  0.6935567259788513\n",
      "acc (%):  tensor(50.)\n"
     ]
    }
   ],
   "source": [
    "real_test(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
