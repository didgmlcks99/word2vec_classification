{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import gensim\n",
    "import simple_net\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_df.sentence.values\n",
    "train_labels = train_df.label.values\n",
    "\n",
    "test_sentences = test_df.sentence.values\n",
    "test_labels = test_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('../model/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bring_nn_input(sentences):\n",
    "    vec_list = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = sentence.strip().split()\n",
    "\n",
    "        count = 0\n",
    "        sum_vec = np.zeros(300)\n",
    "\n",
    "        for word in words:\n",
    "            if word in word2vec_model:\n",
    "                sum_vec += word2vec_model[word]\n",
    "                count += 1\n",
    "        \n",
    "        if count != 0:\n",
    "            vec = (sum_vec / count)\n",
    "    \n",
    "        vec_list.append(vec)\n",
    "\n",
    "    return np.array(vec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = bring_nn_input(train_sentences)\n",
    "test_vec = bring_nn_input(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 300\n",
    "hidden=100\n",
    "output = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_model = simple_net.Simple_Net(input, hidden, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(train_vec, train_labels, shuffle=True, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "optimizer = torch.optim.Adam(sn_model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/usairline{}'.format(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sn_train(epochs):\n",
    "    x_train = Variable(torch.from_numpy(features_train)).float()\n",
    "    y_train = Variable(torch.from_numpy(labels_train)).long()\n",
    "\n",
    "    x_test = Variable(torch.from_numpy(features_test)).float()\n",
    "    y_test = Variable(torch.from_numpy(labels_test)).long()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        sn_model.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred_train = sn_model(x_train)\n",
    "        loss_train = criterion(y_pred_train, y_train)\n",
    "        \n",
    "        # print (\"epoch #\",epoch)\n",
    "        print (\"train loss: \", loss_train.item())\n",
    "        pred_train = torch.max(y_pred_train, 1)[1].eq(y_train).sum()\n",
    "        print (\"train acc:(%) \", 100*pred_train/len(x_train))\n",
    "\n",
    "        tb_x = epoch\n",
    "        # writer.add_scalar('Loss/train', loss_train.item(), tb_x)\n",
    "\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        sn_model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred_test = sn_model(x_test)\n",
    "            loss_test = criterion(y_pred_test, y_test)\n",
    "            \n",
    "            # print (\"epoch #\",epoch)\n",
    "            print (\"test loss: \", loss_test.item())\n",
    "            pred_test = torch.max(y_pred_test, 1)[1].eq(y_test).sum()\n",
    "            print (\"test acc (%): \", 100*pred_test/len(x_test))\n",
    "        \n",
    "        writer.add_scalars('SimpleNet Training vs. Testing Loss',\n",
    "                    { 'Train' : loss_train.item(), 'Test' : loss_test.item() },\n",
    "                    tb_x + 1)\n",
    "        \n",
    "        writer.add_scalars('SimpleNet Training vs. Testing Accuracy',\n",
    "                    { 'Train' : 100*pred_train/len(x_train), 'Test' : 100*pred_test/len(x_test) },\n",
    "                    tb_x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sn_test(epochs):\n",
    "    sn_model.eval()\n",
    "    x_test = Variable(torch.from_numpy(features_test)).float()\n",
    "    y_test = Variable(torch.from_numpy(labels_test)).long()\n",
    "    for epoch in range(epochs):\n",
    "        with torch.no_grad():\n",
    "            y_pred = sn_model(x_test)\n",
    "            loss = criterion(y_pred, y_test)\n",
    "            print (\"epoch #\",epoch)\n",
    "            print (\"loss: \", loss.item())\n",
    "            pred = torch.max(y_pred, 1)[1].eq(y_test).sum()\n",
    "            print (\"acc (%): \", 100*pred/len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sn_real_test(epochs):\n",
    "    sn_model.eval()\n",
    "    x_test = Variable(torch.from_numpy(test_vec)).float()\n",
    "    y_test = Variable(torch.from_numpy(test_labels)).long()\n",
    "    for epoch in range(epochs):\n",
    "        with torch.no_grad():\n",
    "            y_pred = sn_model(x_test)\n",
    "            loss = criterion(y_pred, y_test)\n",
    "            print (\"epoch #\",epoch)\n",
    "            print (\"loss: \", loss.item())\n",
    "            pred = torch.max(y_pred, 1)[1].eq(y_test).sum()\n",
    "            print (\"acc (%): \", 100*pred/len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  0.6832089424133301\n",
      "train acc:(%)  tensor(62.2362)\n",
      "test loss:  0.6639269590377808\n",
      "test acc (%):  tensor(61.7978)\n",
      "train loss:  0.6621044874191284\n",
      "train acc:(%)  tensor(62.2362)\n",
      "test loss:  0.805731475353241\n",
      "test acc (%):  tensor(61.7978)\n",
      "train loss:  0.7948875427246094\n",
      "train acc:(%)  tensor(62.2362)\n",
      "test loss:  0.6558749079704285\n",
      "test acc (%):  tensor(61.7978)\n",
      "train loss:  0.6533856987953186\n",
      "train acc:(%)  tensor(62.2362)\n",
      "test loss:  0.6750422716140747\n",
      "test acc (%):  tensor(63.9627)\n",
      "train loss:  0.6738559007644653\n",
      "train acc:(%)  tensor(64.7575)\n",
      "test loss:  0.6658895611763\n",
      "test acc (%):  tensor(68.0186)\n",
      "train loss:  0.6637394428253174\n",
      "train acc:(%)  tensor(69.7177)\n",
      "test loss:  0.65205317735672\n",
      "test acc (%):  tensor(70.8413)\n",
      "train loss:  0.6485337018966675\n",
      "train acc:(%)  tensor(72.3486)\n",
      "test loss:  0.6339306831359863\n",
      "test acc (%):  tensor(71.6087)\n",
      "train loss:  0.6293730139732361\n",
      "train acc:(%)  tensor(72.1019)\n",
      "test loss:  0.6165004372596741\n",
      "test acc (%):  tensor(73.8559)\n",
      "train loss:  0.6105536222457886\n",
      "train acc:(%)  tensor(75.1165)\n",
      "test loss:  0.6179541945457458\n",
      "test acc (%):  tensor(72.5952)\n",
      "train loss:  0.6145994663238525\n",
      "train acc:(%)  tensor(72.7779)\n",
      "test loss:  0.6161558628082275\n",
      "test acc (%):  tensor(70.7043)\n",
      "train loss:  0.6109288930892944\n",
      "train acc:(%)  tensor(72.1202)\n",
      "test loss:  0.6264700889587402\n",
      "test acc (%):  tensor(68.7860)\n",
      "train loss:  0.6208529472351074\n",
      "train acc:(%)  tensor(69.7725)\n",
      "test loss:  0.6125989556312561\n",
      "test acc (%):  tensor(70.1836)\n",
      "train loss:  0.6052029728889465\n",
      "train acc:(%)  tensor(71.1885)\n",
      "test loss:  0.5727899074554443\n",
      "test acc (%):  tensor(74.7602)\n",
      "train loss:  0.5655385851860046\n",
      "train acc:(%)  tensor(75.2535)\n",
      "test loss:  0.6139414310455322\n",
      "test acc (%):  tensor(69.7999)\n",
      "train loss:  0.6108443737030029\n",
      "train acc:(%)  tensor(70.1927)\n",
      "test loss:  0.5376980900764465\n",
      "test acc (%):  tensor(75.8290)\n",
      "train loss:  0.5285225510597229\n",
      "train acc:(%)  tensor(76.5507)\n",
      "test loss:  0.5600734353065491\n",
      "test acc (%):  tensor(72.6500)\n",
      "train loss:  0.549419641494751\n",
      "train acc:(%)  tensor(73.5453)\n",
      "test loss:  0.5619205236434937\n",
      "test acc (%):  tensor(72.7597)\n",
      "train loss:  0.5483012795448303\n",
      "train acc:(%)  tensor(73.7828)\n",
      "test loss:  0.5215135216712952\n",
      "test acc (%):  tensor(75.8016)\n",
      "train loss:  0.5047460794448853\n",
      "train acc:(%)  tensor(76.8430)\n",
      "test loss:  0.4914877414703369\n",
      "test acc (%):  tensor(77.4185)\n",
      "train loss:  0.48360753059387207\n",
      "train acc:(%)  tensor(78.0488)\n",
      "test loss:  0.5077716708183289\n",
      "test acc (%):  tensor(75.5549)\n",
      "train loss:  0.5050854086875916\n",
      "train acc:(%)  tensor(75.4910)\n",
      "test loss:  0.48090019822120667\n",
      "test acc (%):  tensor(78.0214)\n",
      "train loss:  0.46548911929130554\n",
      "train acc:(%)  tensor(79.0536)\n",
      "test loss:  0.5116720199584961\n",
      "test acc (%):  tensor(76.4319)\n",
      "train loss:  0.4880441725254059\n",
      "train acc:(%)  tensor(77.9848)\n",
      "test loss:  0.5017771124839783\n",
      "test acc (%):  tensor(76.9252)\n",
      "train loss:  0.4773118495941162\n",
      "train acc:(%)  tensor(78.3228)\n",
      "test loss:  0.47058725357055664\n",
      "test acc (%):  tensor(78.4599)\n",
      "train loss:  0.45131832361221313\n",
      "train acc:(%)  tensor(79.6657)\n",
      "test loss:  0.48162001371383667\n",
      "test acc (%):  tensor(77.4185)\n",
      "train loss:  0.46778663992881775\n",
      "train acc:(%)  tensor(77.8935)\n",
      "test loss:  0.4649598002433777\n",
      "test acc (%):  tensor(78.9257)\n",
      "train loss:  0.4447702169418335\n",
      "train acc:(%)  tensor(79.8118)\n",
      "test loss:  0.4675646424293518\n",
      "test acc (%):  tensor(78.2954)\n",
      "train loss:  0.43623122572898865\n",
      "train acc:(%)  tensor(80.4787)\n",
      "test loss:  0.4819839596748352\n",
      "test acc (%):  tensor(77.9940)\n",
      "train loss:  0.44395360350608826\n",
      "train acc:(%)  tensor(80.0402)\n",
      "test loss:  0.46682992577552795\n",
      "test acc (%):  tensor(78.6791)\n",
      "train loss:  0.4293482005596161\n",
      "train acc:(%)  tensor(80.5335)\n",
      "test loss:  0.46052759885787964\n",
      "test acc (%):  tensor(78.9805)\n",
      "train loss:  0.4277796447277069\n",
      "train acc:(%)  tensor(81.0816)\n",
      "test loss:  0.4580315947532654\n",
      "test acc (%):  tensor(79.3368)\n",
      "train loss:  0.4262874722480774\n",
      "train acc:(%)  tensor(81.2186)\n",
      "test loss:  0.45023590326309204\n",
      "test acc (%):  tensor(79.1450)\n",
      "train loss:  0.4146125018596649\n",
      "train acc:(%)  tensor(81.6845)\n",
      "test loss:  0.4532683491706848\n",
      "test acc (%):  tensor(78.7065)\n",
      "train loss:  0.41371646523475647\n",
      "train acc:(%)  tensor(81.1821)\n",
      "test loss:  0.4506780505180359\n",
      "test acc (%):  tensor(78.8709)\n",
      "train loss:  0.4112809896469116\n",
      "train acc:(%)  tensor(81.3922)\n",
      "test loss:  0.4433429539203644\n",
      "test acc (%):  tensor(79.8849)\n",
      "train loss:  0.40758568048477173\n",
      "train acc:(%)  tensor(81.4835)\n",
      "test loss:  0.4392726719379425\n",
      "test acc (%):  tensor(79.9397)\n",
      "train loss:  0.4019811153411865\n",
      "train acc:(%)  tensor(81.8215)\n",
      "test loss:  0.43910476565361023\n",
      "test acc (%):  tensor(79.6657)\n",
      "train loss:  0.3971112072467804\n",
      "train acc:(%)  tensor(82.1412)\n",
      "test loss:  0.4395822584629059\n",
      "test acc (%):  tensor(79.8849)\n",
      "train loss:  0.3937877118587494\n",
      "train acc:(%)  tensor(82.3239)\n",
      "test loss:  0.4396415948867798\n",
      "test acc (%):  tensor(80.3782)\n",
      "train loss:  0.3904431462287903\n",
      "train acc:(%)  tensor(82.4975)\n",
      "test loss:  0.4395996928215027\n",
      "test acc (%):  tensor(80.4330)\n",
      "train loss:  0.3872041702270508\n",
      "train acc:(%)  tensor(82.5706)\n",
      "test loss:  0.43818768858909607\n",
      "test acc (%):  tensor(80.4604)\n",
      "train loss:  0.3836396336555481\n",
      "train acc:(%)  tensor(82.7807)\n",
      "test loss:  0.43334969878196716\n",
      "test acc (%):  tensor(80.6796)\n",
      "train loss:  0.3801320195198059\n",
      "train acc:(%)  tensor(82.9816)\n",
      "test loss:  0.4308689832687378\n",
      "test acc (%):  tensor(80.7070)\n",
      "train loss:  0.3765164613723755\n",
      "train acc:(%)  tensor(83.1917)\n",
      "test loss:  0.428906112909317\n",
      "test acc (%):  tensor(81.1729)\n",
      "train loss:  0.3733689785003662\n",
      "train acc:(%)  tensor(83.2831)\n",
      "test loss:  0.4278200566768646\n",
      "test acc (%):  tensor(81.3099)\n",
      "train loss:  0.37023892998695374\n",
      "train acc:(%)  tensor(83.5297)\n",
      "test loss:  0.4287993013858795\n",
      "test acc (%):  tensor(81.2825)\n",
      "train loss:  0.3665158450603485\n",
      "train acc:(%)  tensor(83.6485)\n",
      "test loss:  0.43069028854370117\n",
      "test acc (%):  tensor(81.2825)\n",
      "train loss:  0.36250773072242737\n",
      "train acc:(%)  tensor(83.8403)\n",
      "test loss:  0.43259868025779724\n",
      "test acc (%):  tensor(81.2003)\n",
      "train loss:  0.3587833642959595\n",
      "train acc:(%)  tensor(84.0961)\n",
      "test loss:  0.43487152457237244\n",
      "test acc (%):  tensor(80.9263)\n",
      "train loss:  0.35520997643470764\n",
      "train acc:(%)  tensor(84.4158)\n",
      "test loss:  0.4330439567565918\n",
      "test acc (%):  tensor(80.9537)\n",
      "train loss:  0.3515528738498688\n",
      "train acc:(%)  tensor(84.2057)\n",
      "test loss:  0.4347243905067444\n",
      "test acc (%):  tensor(81.1181)\n",
      "train loss:  0.3479922115802765\n",
      "train acc:(%)  tensor(84.8452)\n",
      "test loss:  0.43261656165122986\n",
      "test acc (%):  tensor(81.2277)\n",
      "train loss:  0.34405791759490967\n",
      "train acc:(%)  tensor(84.8908)\n",
      "test loss:  0.43169957399368286\n",
      "test acc (%):  tensor(81.2277)\n",
      "train loss:  0.33981427550315857\n",
      "train acc:(%)  tensor(85.1740)\n",
      "test loss:  0.4326533079147339\n",
      "test acc (%):  tensor(81.3922)\n",
      "train loss:  0.33597853779792786\n",
      "train acc:(%)  tensor(85.4024)\n",
      "test loss:  0.43554142117500305\n",
      "test acc (%):  tensor(81.5018)\n",
      "train loss:  0.3322718143463135\n",
      "train acc:(%)  tensor(85.5394)\n",
      "test loss:  0.4437790513038635\n",
      "test acc (%):  tensor(81.4196)\n",
      "train loss:  0.3302386701107025\n",
      "train acc:(%)  tensor(85.5942)\n",
      "test loss:  0.4466165602207184\n",
      "test acc (%):  tensor(81.4196)\n",
      "train loss:  0.3305648863315582\n",
      "train acc:(%)  tensor(85.4024)\n",
      "test loss:  0.4602145254611969\n",
      "test acc (%):  tensor(80.8715)\n",
      "train loss:  0.33401724696159363\n",
      "train acc:(%)  tensor(85.4663)\n",
      "test loss:  0.4436643123626709\n",
      "test acc (%):  tensor(81.5292)\n",
      "train loss:  0.3235754072666168\n",
      "train acc:(%)  tensor(85.9414)\n",
      "test loss:  0.4414365887641907\n",
      "test acc (%):  tensor(81.9403)\n",
      "train loss:  0.3166298270225525\n",
      "train acc:(%)  tensor(86.2519)\n",
      "test loss:  0.45472922921180725\n",
      "test acc (%):  tensor(81.0085)\n",
      "train loss:  0.3183886706829071\n",
      "train acc:(%)  tensor(85.8865)\n",
      "test loss:  0.4535974860191345\n",
      "test acc (%):  tensor(81.6388)\n",
      "train loss:  0.3134112060070038\n",
      "train acc:(%)  tensor(86.2063)\n",
      "test loss:  0.45993438363075256\n",
      "test acc (%):  tensor(81.6662)\n",
      "train loss:  0.3064229190349579\n",
      "train acc:(%)  tensor(86.4803)\n",
      "test loss:  0.4655303657054901\n",
      "test acc (%):  tensor(81.8306)\n",
      "train loss:  0.3026711344718933\n",
      "train acc:(%)  tensor(86.8183)\n",
      "test loss:  0.46939796209335327\n",
      "test acc (%):  tensor(81.8032)\n",
      "train loss:  0.30195751786231995\n",
      "train acc:(%)  tensor(86.6996)\n",
      "test loss:  0.4802284836769104\n",
      "test acc (%):  tensor(81.3648)\n",
      "train loss:  0.30354568362236023\n",
      "train acc:(%)  tensor(86.7635)\n",
      "test loss:  0.4728812873363495\n",
      "test acc (%):  tensor(81.9951)\n",
      "train loss:  0.3022468388080597\n",
      "train acc:(%)  tensor(86.9188)\n",
      "test loss:  0.47837579250335693\n",
      "test acc (%):  tensor(81.8032)\n",
      "train loss:  0.2968674898147583\n",
      "train acc:(%)  tensor(87.0924)\n",
      "test loss:  0.4719833731651306\n",
      "test acc (%):  tensor(81.6662)\n",
      "train loss:  0.2882859408855438\n",
      "train acc:(%)  tensor(87.3664)\n",
      "test loss:  0.48022204637527466\n",
      "test acc (%):  tensor(81.4470)\n",
      "train loss:  0.28649964928627014\n",
      "train acc:(%)  tensor(87.4212)\n",
      "test loss:  0.5053415298461914\n",
      "test acc (%):  tensor(81.1181)\n",
      "train loss:  0.2937041223049164\n",
      "train acc:(%)  tensor(87.1654)\n",
      "test loss:  0.5178819298744202\n",
      "test acc (%):  tensor(80.3234)\n",
      "train loss:  0.3117595911026001\n",
      "train acc:(%)  tensor(85.5942)\n",
      "test loss:  0.5447956323623657\n",
      "test acc (%):  tensor(80.3508)\n",
      "train loss:  0.32247501611709595\n",
      "train acc:(%)  tensor(85.9231)\n",
      "test loss:  0.479818195104599\n",
      "test acc (%):  tensor(81.9951)\n",
      "train loss:  0.2822149395942688\n",
      "train acc:(%)  tensor(87.6861)\n",
      "test loss:  0.49681034684181213\n",
      "test acc (%):  tensor(80.9537)\n",
      "train loss:  0.3101997971534729\n",
      "train acc:(%)  tensor(86.6265)\n",
      "test loss:  0.48304691910743713\n",
      "test acc (%):  tensor(81.7210)\n",
      "train loss:  0.2816144526004791\n",
      "train acc:(%)  tensor(87.4395)\n",
      "test loss:  0.5135692358016968\n",
      "test acc (%):  tensor(81.0633)\n",
      "train loss:  0.29447874426841736\n",
      "train acc:(%)  tensor(87.0010)\n",
      "test loss:  0.5077637434005737\n",
      "test acc (%):  tensor(81.7484)\n",
      "train loss:  0.279959499835968\n",
      "train acc:(%)  tensor(87.5400)\n",
      "test loss:  0.5187901854515076\n",
      "test acc (%):  tensor(81.7484)\n",
      "train loss:  0.2709755003452301\n",
      "train acc:(%)  tensor(87.9328)\n",
      "test loss:  0.5576281547546387\n",
      "test acc (%):  tensor(81.0907)\n",
      "train loss:  0.2843133807182312\n",
      "train acc:(%)  tensor(87.5217)\n",
      "test loss:  0.535436749458313\n",
      "test acc (%):  tensor(81.7210)\n",
      "train loss:  0.2657558023929596\n",
      "train acc:(%)  tensor(88.2708)\n",
      "test loss:  0.5365540385246277\n",
      "test acc (%):  tensor(81.6388)\n",
      "train loss:  0.2697863280773163\n",
      "train acc:(%)  tensor(87.9236)\n",
      "test loss:  0.5425140857696533\n",
      "test acc (%):  tensor(81.8306)\n",
      "train loss:  0.26791396737098694\n",
      "train acc:(%)  tensor(88.1246)\n",
      "test loss:  0.5270553827285767\n",
      "test acc (%):  tensor(81.8306)\n",
      "train loss:  0.2568197250366211\n",
      "train acc:(%)  tensor(88.8463)\n",
      "test loss:  0.5354738235473633\n",
      "test acc (%):  tensor(81.4744)\n",
      "train loss:  0.2666284441947937\n",
      "train acc:(%)  tensor(88.0424)\n",
      "test loss:  0.5479772686958313\n",
      "test acc (%):  tensor(81.3648)\n",
      "train loss:  0.2557733654975891\n",
      "train acc:(%)  tensor(88.7184)\n",
      "test loss:  0.5618434548377991\n",
      "test acc (%):  tensor(81.4470)\n",
      "train loss:  0.24968509376049042\n",
      "train acc:(%)  tensor(88.9193)\n",
      "test loss:  0.585072934627533\n",
      "test acc (%):  tensor(81.3099)\n",
      "train loss:  0.25724369287490845\n",
      "train acc:(%)  tensor(88.5357)\n",
      "test loss:  0.6037843823432922\n",
      "test acc (%):  tensor(81.3099)\n",
      "train loss:  0.2521003484725952\n",
      "train acc:(%)  tensor(88.9376)\n",
      "test loss:  0.5941105484962463\n",
      "test acc (%):  tensor(81.5566)\n",
      "train loss:  0.23977403342723846\n",
      "train acc:(%)  tensor(89.5953)\n",
      "test loss:  0.6035905480384827\n",
      "test acc (%):  tensor(81.3922)\n",
      "train loss:  0.24544817209243774\n",
      "train acc:(%)  tensor(89.0472)\n",
      "test loss:  0.6302750706672668\n",
      "test acc (%):  tensor(81.0633)\n",
      "train loss:  0.24994689226150513\n",
      "train acc:(%)  tensor(88.9193)\n",
      "test loss:  0.6177486181259155\n",
      "test acc (%):  tensor(81.2551)\n",
      "train loss:  0.235813170671463\n",
      "train acc:(%)  tensor(89.6136)\n",
      "test loss:  0.6318240165710449\n",
      "test acc (%):  tensor(81.5840)\n",
      "train loss:  0.22932401299476624\n",
      "train acc:(%)  tensor(89.7872)\n",
      "test loss:  0.6663264036178589\n",
      "test acc (%):  tensor(81.1729)\n",
      "train loss:  0.23650586605072021\n",
      "train acc:(%)  tensor(89.5953)\n",
      "test loss:  0.6779829859733582\n",
      "test acc (%):  tensor(80.7345)\n",
      "train loss:  0.23878368735313416\n",
      "train acc:(%)  tensor(89.4400)\n",
      "test loss:  0.6847859025001526\n",
      "test acc (%):  tensor(80.9537)\n",
      "train loss:  0.24154308438301086\n",
      "train acc:(%)  tensor(89.2756)\n",
      "test loss:  0.6496179699897766\n",
      "test acc (%):  tensor(80.9263)\n",
      "train loss:  0.22816626727581024\n",
      "train acc:(%)  tensor(90.1525)\n",
      "test loss:  0.6492555141448975\n",
      "test acc (%):  tensor(81.4196)\n",
      "train loss:  0.2170841544866562\n",
      "train acc:(%)  tensor(90.4723)\n",
      "test loss:  0.6868212819099426\n",
      "test acc (%):  tensor(81.0359)\n"
     ]
    }
   ],
   "source": [
    "sn_train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch # 0\n",
      "loss:  0.6868212819099426\n",
      "acc (%):  tensor(81.0359)\n"
     ]
    }
   ],
   "source": [
    "sn_test(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch # 0\n",
      "loss:  0.5629386305809021\n",
      "acc (%):  tensor(82.5000)\n"
     ]
    }
   ],
   "source": [
    "sn_real_test(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
